In this section we will describe the different algorithms and memory
layouts we have used.

\subsection{Simple multiplication}

\subsubsection{Row-based layout}
We started out implementing the simple conventional $O(n^3)$ matrix
multiplication algorithm where we stored the matrices using a row
based layout.

In order to estimate the number of cache faults, we will for each row
in the output matrix try to bound the number of cache faults
encountered in order to compute it. Figure~\ref{fig:rowrowmul}
illustrates the layouts and the multiplication process.
\begin{figure}[h!]
  \centering
  \includegraphics[width=8cm]{images/rowrowmul}
  \label{fig:rowrowmul}
  \caption{Illustration of multiplication with row-row layout.}
\end{figure}

We want to compute the matrix product $C = AB$. We do this by filling
out all entries in $C$ in a row-by-row and inside a row,
column-by-column fashion. Hence, if the matrices are sufficiently
small, then the current row of $A$ will never be excluded from
cache. In order for this to happen, we should have enough cache to
store all the cache lines used in matrix $B$ (to fill the current
row). These are the cache lines marked with the darker blue color in
Figure~\ref{fig:rowrowmul}. Because the CPU uses an adjacent cache
line prefetcher, we also expect it to load the next cache line, hence
we also expect the area marked with light blue to be in cache. If we
let $\mathcal{M}$ denote the size of the cache in bytes, then we find
that we can reuse the cache lines of $C$ only if
\[
( n + n \cdot \underbrace{8}_{\mathclap{8 \text{ doubles in a cache line}}} \cdot 2) \cdot 8 \leq \mathcal{M}.
\iff
n \leq \frac{\mathcal{M}}{136}.
\]
Since the cache works in a LRU like fashion[citation?], we will get no
benefit from the loaded cache lines of the $B$ matrix if the cached
elements does not fit. For example, the L2 cache, which is $256kb$,
will begin to trash the values when
\[
n \leq \frac{256 \cdot 1024}{136} \approx 1927.
\]
We will analyze the expected number of cache faults in both situations.

In order to estimate the number of cache faults, we will consider the
number of cache faults incurred when computing one line in the $C$
matrix. When we begin to calculate the next line, we will not be able
to reuse any cache data, since we start computation from the first
column.



 We assume that the cache is reset
after the computation of a row\todo{Argue why this makes sense}.

\todo{Insert image illustrating this!}

When computing a row in $C = AB$, we keep a row in $A$ fixed, and vary
the columns in $B$. Hence the row i $A$ will always be in cache,
initially causing $\frac{n}{B}$ cache faults. We compute all columns
in the result row sequentially. Because our cache is big enough to
hold $n+1$ cache lines\todo{Argue!?}, we can hold all cache lines


\todo{Expectations of cache faults here....}

We do not expect any significant amount of branch mispredictions.

\subsubsection{Combined row-based and column-based layout}

In order to improve the number of cache faults, we have tried to use a
column base layout in the right operand in the multiplication. We
expect this to give us a bit better cache performance. This approach
has the drawback of limiting a matrix only to be used on one side of a
multiplication. However, this problem can be mitigated by conversions which is an $O(n^2)$ operation.

We analyze the number as cache faults as before, but 

\subsection{Recursive multiplication}

For exploiting more kinds of layouts we implemented the recursive algorithm.

\subsubsection{Z-curve layout}

The first idea was to use a Z-curved layout. One major advantage of this layout is that improves locality on all levels of the recursive multiplication. The drawback is that the index calculations are time consuming. On x86 we get approximately 50 bitwise operations each time we want to convert a coordinate to the position in the array. However, this can be improved by incrementally constructing the Z-curve numbers or by precomputing offsets at base cases. The upcoming Intel Haswell architecture has support for bit permutations which will improve the situation.

We ended up with precomputing the Z-curve offsets at the base case. That means the only penalty when we want to store or lookup a value is a level of indirection. In the recursive algorithm we used 8*8 as a base case for switching to the naive algorithm.

\subsubsection{Tiled layout}

A tiled layout is a compromise between locality and performance of index calculations. When the recursive algorithm reaches blocks of the same size as the tiles, it switches to the naive algorithm. And the naive algorithm functions without any modifications. To improve cache locality a bit we used a row-based layout in the tiles for the left operand and a column-based layout in the tiles for the right operand.

\subsection{Strassen}

Strassen exploits some tricks with matrix multiplication such that it only uses 7 multiplications instead of 8 multiplications. It is a recursive algorithm by nature. The first step of the algorithm is to split the operands into 2*2 blocked matrix.

Instead of actually splitting the matrix into smaller matrices we have used a Z-curve layout. That means that the smaller matrices are able to just point (instead of actually copying) to the data in the parent matrix while still preserving a proper layout themselves.

Strassen needs to have a large base case such that we do not spend too much time on the recursion and to improve cache locality at the base levels. To avoid the overhead of handling matrix multiplication with a level of indirection or by calculating Z-curve indices, we used a row-based and column-based layout at the base case for the left and right operand respectively.

The additions and subtractions in the algorithm only works on left operand block matrices or right operand block matrices. Therefore, we can use a single loop and no index calculations to add and subtract matrices. I.e. Z-curve indices are the same and we only add/subtract row-based base cases with row-based base cases etc.

One major drawback of the algorithm is that we have to allocate 7 temporary matrices of one fourth of the size of the resulting matrix + at least one extra used by the calculation of the temporary matrices. Therefore, a two input matrices of size 1024 * 1024 containing double precision floating points needs ~5*8 MB instead of 3*8 MB.

One improvement we have made to the algorithm is to allocate all the temporary matrices in a stack-like fashion such that we reduce the number of cache evictions when we start writing at new locations instead of reusing locations already in cache.

Furthermore, one problem we experienced with algorithm was numerical instability.

\todo{Manglende praecision, analyse}
% http://software.intel.com/en-us/articles/performance-insights-to-intel-hyper-threading-technology

\subsection{SIMD instructions}

...

\subsection{Parallelization}

\todo{SIMD giver ikke ligesaa meget naar parallelet. Maaske giver hyper-threading ligesaa meget}

\subsubsection{Combined row-based and column-based layout}

The naive algorithm which is used for this combined is easily parallelizable because we can assign intervals of rows for each thread. The result matrix is stored in a row-based layout so writing is separated so cache thrashing should not be a problem.

...

\todo{todo, mere l2 cache}

\todo{formelen for parallelization}

\subsubsection{Strassen}

Strassen was parallelized by starting new threads at each of the 7 multiplications. This was done at 1 or 2 levels depending on the level of parallization we wanted. The last additions/subtractions and combine operations were not parallelized. However, the multiplications uses most of the time.

One problem Strassen has compared to other algorithms is that it allocates temporary matrices. This problem gets more severe with parallization because now each thread allocates temporary matrices and therefore the memory consumption is higher than the sequential version.

Another problem we had was that our custom stack-like allocator was not thread-safe so it is disabled for parallization. A simple lock-based approach yielded too much congestion. One suggestion to improve this is to give each thread their own allocator.

\todo{Loesning: Maaske start med at genere en pool af stack allocators. En ny traad kan saa nappe en af dem. Paa den maade betaler vi ikke saa meget for opstarten som vi ellers ville have gjort.}
