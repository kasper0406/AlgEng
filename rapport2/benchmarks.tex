\subsection{Test setup}

Our program was written in C/C++ and compiled with GCC. All memory allocations were cache line boundary aligned. For parallelization we used C++11 cross-platform library \texttt{<thread>}.

All benchmarks were performed on a Linux desktop which has 16 GB ram and a Core i7 3770 CPU with the following specification:

\begin{itemize}
\item 4 * 3.4 GHz (Turbo Boost up to 3.9 GHz)
\item 4 * 32 kB L1 instruction cache
\item 4 * 32 kB L1 data cache (write back)
\item 4 * 256 kB L2 cache (write back)
\item Hyper-Threading
\item Shared 8 MB L3 cache (inclusive, write back)
\item 64 byte cache lines
\item The associativity for the cache levels are 8, 8 and 16 respectively
% http://www.ni.com/white-paper/11266/en#toc5
\end{itemize}

L2 cache faults, L3 cache faults and retired instructions were measured using Intel Performance Monitor Counter.

The data in the matrices were randomly generated double precision floating points with
an uniform distribution. The range of the data was -1 to
1.

\subsection{Simple multiplication}

\subsubsection{Row-based layout}
Figure~\ref{fig:rnrnrn0} shows the measured running time and cache
faults encountered when running the naive matrix multiplier on
matrices with row layout.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{plots/rowrow.pdf}
  \caption{Running time and cache faults of naive matrix
    multiplication with all matrices using row layout.}
  \label{fig:rnrnrn0}
\end{figure}

From figure~\ref{fig:rnrnrn0} it seems like we typically gets a bit fewer
cache faults than we expect which can be explained by prefetching. The jumps we expect
also seems to happen, although in the experiments, they seem to happen
a bit before we expect. This can be explained by that the cache also
contains some data which is not the matrices.

Figure~\ref{fig:rnrnrn0} moreover shows a lot of spikes on the lines
indicating L2 and L3 cache faults. The L2 spikes is beginning around
$n = 256$ and the L3 spikes around $n = 1024$.

We strongly suspect that the cache layout due to the 8-way
associativity is to blame. When looking up in the L2 cache, the 36 bit
physical address of the i7 processor is split into three parts $addr =
(tag,index,offset)$. Since the cache line size is 64 B, $6$ bits
should be used to reference a byte in a cache line. Because the L2
cache is 256 kB, we find that the index size to be
\[
  \frac{256 \cdot 1024}{\underbrace{64}_{\text{Cache line size}} \cdot \underbrace{8}_{\text{Cache associativity}}}
    = 512 = 2^9.
\]
Hence, we should use $9$ bits to specify the index. The remaining
$36-6-9=21$ bits is used as tag. Notice that the tag is the most
significant part of the address.

Because the size of our matrices is always divisible by the cache line
size, it will always be the case that a new row in a matrix starts
with a new cache line. But this means that a cache line, and the
corresponding cache lines on the next row, always will be
$\frac{n}{B}$ cache lines apart. Therefore we are only able to utilize
$\ord(\frac{n}{B})$, cache lines in the L2 cache, where $\ord(\cdot)$
denotes the order of the element in the additive group
$\mathbb{Z}_{2^9}$.

Figure~\ref{fig:rowrow_cachepeaks} illustrates the correlation between
these available cache lines (the green line), and the
cache faults divided by the expected number of cache faults (the red
line).

When comparing the interval from $[7.5; 9]$ with the interval $[9;
  10]$, the green line seems to dictate significant more peeks in the
$[9; 10]$ interval. This also makes sense, since the matrices in this
case are bigger, meaning that more columns of the $B$ matrix will be
loaded. Therefore we will overwrite the same available cache slots
more times (we can afford to do this 8 time, since we have an 8-way
cache). The plot on figure~\ref{fig:rowrow_cachepeaks} heavily suggest
that these peaks actually stems from the 8-way cache layout.

\begin{figure}[h!]
  \centering
  \includegraphics{plots/rowrow_cachepeaks}
  \caption{The number of available cache lines vs. the number of cache
    faults occurring in experiments.}
  \label{fig:rowrow_cachepeaks}
\end{figure}

It can be seen from figure~\ref{fig:rnrnrn0} that the running time
becomes a lot worse at $2^9 = 512$. At this points when we jump from
one row to another in the $B$ matrix, we jump a distance of $512 \cdot
8 = 4096$ bytes. This is the page size of the system, so we jump
exactly a page for every read. Moreover, the size of the second-level
TLB in the i7 processor can hold $512$ entries and uses a pseudo-LRU
replacement strategy, leading to a TLB miss every time we read a cell
in the $B$ matrix. This explains why we see the running time becomes
worse at this point.

\subsubsection{Row/column-based layout}
Next we performed experiments where we stored the $A$ matrix in row
form, the $B$ matrix in column form and saved the result in matrix $C$
in row form. From the algorithms section, we believe this to perform
better than the row/row naive multiplication.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{plots/rowcol}
  \caption{Running time and cache faults using naive matrix
    multiplication with row and column layout.}
  \label{fig:rowcol}
\end{figure}

One thing to notice in figure~\ref{fig:rowcol} is that the number of
L2 cache faults seems to follow the number of L3 cache faults. This
was not what we expected. As shown by the purple line in the plot, we
would expect the number of L2 cache faults to become significantly
worse at input size $n \approx 180$, when the $B$ matrix does not fit
the cache anymore.

In order to explain this behavior we have tried to disassemble the
program to verify that the compiler did not make any very clever
unexpected optimizations. This turned out not to be the case.

We then tried to make a simple model of the i7 L2 cache, where we
simulated the 8-way cache layout\footnote{The simulation was based on
  the description of the i7 L2 cache described in \cite[p. 117 - 121]{Hanne}. We also tried the simulation on the row/row layout, this
  corresponded quite nicely with the real world, hinting some sanity
  of the simulation.}. In this experiment, we also tried to fill out
the matrix one row at a time, but start from the right end half of the
times. This should give us a better cache
performance. Figure~\ref{fig:rowcol_simulation} shows the result of
this simulation compared to the actual number of cache faults.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{plots/rowcol_simulation}
  \caption{Simulated cache performance vs. the actual cache performance.}
  \label{fig:rowcol_simulation}
\end{figure}

As figure~\ref{fig:rowcol_simulation} illustrates, the simulated cache
performance very nicely follows our expectations. Changing the
for-loop so that we start filling out from the right half of the times
helps a bit on the cache performance, but can not explain the
real-life L2 cache performance.

This hints that there is some hardware in the CPU, that in this case
is doing a really good job to minimize the L2 cache faults. We tried
to run the experiments on a smaller computer with an i3-550 processor
where we disabled the hardware- and adjacent cache line
prefetchers. This test, however, turned out inconclusive\footnote{We
  got some very strange looking results from the test. The test was
  performed running a live linux distribution booted from an USB
  storage stick, but the stick broke during the next restart of the
  machine. Hence, if our code made some call to the operating system
  not loaded into memory, our results could be heavily influenced by
  malfunctioning hardware. Therefore we do not have a lot of
  confidence in the results.}.

Figure~\ref{fig:rowcol_vs_rowrow} is comparing the row/column results
to the row/row results. This plot shows the row/row results divided by
the row/column results. From the algorithms section, we will expect
the improvement factor to be greater than $1$.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{plots/rowcol_vs_rowrow}
  \caption{Relative performance of row/row vs. row/column
    layout. Improvement factor indicates how many times row/column
    performs better than row/row.}
  \label{fig:rowcol_vs_rowrow}
\end{figure}

It can be seen that the row/column version begins to perform a lot
better at $n \approx 2^9$, which is where the row/row layout begin to
experience lots of TLB misses.

The plots shows an improved cache performance and running time of the
row/column layout. However, the plot also shows that the row/row
layout begins to perform more instructions when the size of the
matrices exceeds $2^{10} = 1024$. We suspect this could be because the
operating system needed to perform some additional work to manage the
virtual memory / paging.

\subsection{Recursive}

Figure \ref{fig:recursive_layout_performance} shows the performance of the two layouts with the best base case size of each. As can be seen on the figure, the best performing layout of the recursive algorithm is the tiled layout. And, as expected, the recursive algorithm outperforms the naive row/column algorithm. This is because the cache locality is better for the recursive algorithm (figure \ref{fig:recursive_layout_cachefaults}) and because we forced the compiler to loop unroll the base case which lead to fewer instructions as seen in figure \ref{fig:recursive_layout_instructions}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{"../project2/gnuplots/recursive_performance"}
  \caption{Performance comparison between different layouts and the naive row/column algorithm.}
  \label{fig:recursive_layout_performance}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{"../project2/gnuplots/recursive_cache"}
  \caption{Cache faults for the recursive and naive algorithm.}
  \label{fig:recursive_layout_cachefaults}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{"../project2/gnuplots/recursive_instructions"}
  \caption{Number of instructions for the recursive and naive algorithm.}
  \label{fig:recursive_layout_instructions}
\end{figure}

\subsubsection{Layouts}

The tiled layout is better at utilizing the cache shown in figure \ref{fig:recursive_layout_cachefaults}. The adjacent cache line prefetcher might have an influence on the amount of cache faults for Z-curve layout because in the base case we do not read in Z-order. This is illustrated in figure \ref{fig:z-order}. On average we get 1 cache fault for each column with a Z-curve layout where in column order the prefetcher would be able to predict the next read reducing the average number of cache faults to 1/2 for each column. Furthermore, the Z-curved layout uses more instructions. Recall that, the Z-curve offsets are precomputed but the extra level of indirection and offset calculations into the precomputed table increases the number of instructions.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.3\textwidth]{images/zlayout_cachefaults}
  \caption{Cache faults when reading a column in Z-curve layout with base case size 8.}
  \label{fig:z-order}
\end{figure}

The best Z-curve base size is much smaller than the best tiled layout base case size. It is a trade-off between overhead with recursion and more cache faults, where we get more cache faults when using larger base case sizes due to the same reasoning as in the previous paragraph.

\subsection{Iterative}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{"../project2/gnuplots/iterative_cache"}
  \caption{Cache faults for the recursive and iterative algorithm.}
  \label{fig:iterative_cache}
\end{figure}

In this section we will compare the iterative and recursive algorithm. Figure \ref{fig:iterative_cache} shows that cache locality is worse for the iterative algorithm compared to the recursive. However, the amount of executed instructions for the iterative algorithm is \textasciitilde 14\% less, leading to about the same running time (within 5\permil).

\subsection{Strassen}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{"../project2/gnuplots/recursive_vs_strassen_performance"}
  \caption{Performance of the best recursive and best Strassen.}
  \label{fig:recursive_vs_strassen_performance}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{"../project2/gnuplots/recursive_vs_strassen_cache"}
  \caption{Cache faults of the recursive and Strassen with the same base case size.}
  \label{fig:recursive_vs_strassen_cache}
\end{figure}

Figure \ref{fig:recursive_vs_strassen_performance} shows that Strassen performs better than the tiled recursive algorithm for all the matrix sizes we have tested. However, figure \ref{fig:recursive_vs_strassen_cache} does not give the expected number of cache faults. The increased number of cache faults might be due to large constants in the big Theta notation.

\subsection{SIMD instructions}

We experienced a substantial performance speedup when SIMD instructions were used (figure \ref{fig:simd}). The increase is in line with our expectations. The amount of cache faults stayed the same in the tests as expected while the cache hits were greatly reduced. One AVX SIMD instruction reads 4 double precision floating point at a time leading to only one cache hit/fault.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{"../project2/gnuplots/simd_performance"}
  \caption{Speedup using SIMD instructions.}
  \label{fig:simd}
\end{figure}

As can be seen, the speedup using SIMD instructions for the recursive algorithm is higher than for Strassen. This is likely due to a larger number of cache faults for Strassen. This makes the proportion of time used on fetching data higher. Moreover in the recursive algorithm a larger part uses SIMD instructions.

\subsection{Parallelization}

Figure \ref{fig:parallel_speedup} shows an increase in performance when the algorithms are parallelized. The parallelization upto 4 physical cores are as expected. Without SIMD instructions Hyper-Threading gives a substantial performance boost. Four cores with Hyper-Threading is performing like 5 or 6 physical cores. However, when SIMD instructions are enabled the speedup is equal to or worse when Hyper-Threading is enabled. The problem is that each core only has one FPU \cite{IntelHT} which is shared between a pair of logical Hyper-Threading cores. This makes the FPU the bottleneck.

The problem is not as bad for the iterative algorithm. This is due to the fact that the iterative is more cache bound as shown in figure \ref{fig:iterative_cache}. Thus Hyper-Threading still gives an advantage, because the execution unit is free when the other HT core is fetching data.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{"../project2/gnuplots/parallel_speedup"}
  \caption{Speedup of parallelized algorithms with and without SIMD instructions.}
  \label{fig:parallel_speedup}
\end{figure}

We have analyzed the theoretical speedup using non-linear regression and Amdahl's law. The percentage of parallelized code in the iterative and recursive algorithms are 98\% while our implementation of Strassen has 89\% parallelized code. This could be improved by parallelizing the last additions and the code for combining submatrices. The projected speedup is shown in figure \ref{fig:amdahl}. The speedup is when SIMD instructions are included. The higher performance of Strassen of the tested matrix sizes does not make up for it being less parallelizable when the number of cores increases. When the number of cores goes to infinity the speedup is 9.8 for Strassen while the speedup for the iterative is 44.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{"../project2/gnuplots/amdahl"}
  \caption{Speedup projection using Amdahl's law.}
  \label{fig:amdahl}
\end{figure}
